{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from train import seed_everything, train_and_evaluate, CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(\"gs://leoraggio-kaggle/amex-default-prediction/data/processed/train_data.parquet\")\n",
    "test_data = pd.read_parquet(\"gs://leoraggio-kaggle/amex-default-prediction/data/processed/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [05:56<00:00,  3.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1365 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.254200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224172\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1357\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.314316\ttraining's amex_metric: 0.782223\tvalid_1's binary_logloss: 0.319517\tvalid_1's amex_metric: 0.768012\n",
      "[1000]\ttraining's binary_logloss: 0.244882\ttraining's amex_metric: 0.795931\tvalid_1's binary_logloss: 0.253634\tvalid_1's amex_metric: 0.776811\n",
      "[1500]\ttraining's binary_logloss: 0.220551\ttraining's amex_metric: 0.809574\tvalid_1's binary_logloss: 0.2333\tvalid_1's amex_metric: 0.783662\n",
      "[2000]\ttraining's binary_logloss: 0.208198\ttraining's amex_metric: 0.821834\tvalid_1's binary_logloss: 0.225505\tvalid_1's amex_metric: 0.787628\n",
      "[2500]\ttraining's binary_logloss: 0.199092\ttraining's amex_metric: 0.833245\tvalid_1's binary_logloss: 0.221439\tvalid_1's amex_metric: 0.790266\n",
      "[3000]\ttraining's binary_logloss: 0.192471\ttraining's amex_metric: 0.84366\tvalid_1's binary_logloss: 0.219662\tvalid_1's amex_metric: 0.791665\n",
      "[3500]\ttraining's binary_logloss: 0.186362\ttraining's amex_metric: 0.853719\tvalid_1's binary_logloss: 0.21835\tvalid_1's amex_metric: 0.792935\n",
      "[4000]\ttraining's binary_logloss: 0.18016\ttraining's amex_metric: 0.863593\tvalid_1's binary_logloss: 0.217308\tvalid_1's amex_metric: 0.79339\n",
      "[4500]\ttraining's binary_logloss: 0.175156\ttraining's amex_metric: 0.873108\tvalid_1's binary_logloss: 0.216757\tvalid_1's amex_metric: 0.793415\n",
      "[5000]\ttraining's binary_logloss: 0.169568\ttraining's amex_metric: 0.881853\tvalid_1's binary_logloss: 0.216258\tvalid_1's amex_metric: 0.79399\n",
      "[5500]\ttraining's binary_logloss: 0.164757\ttraining's amex_metric: 0.890882\tvalid_1's binary_logloss: 0.215906\tvalid_1's amex_metric: 0.794426\n",
      "[6000]\ttraining's binary_logloss: 0.160215\ttraining's amex_metric: 0.898579\tvalid_1's binary_logloss: 0.215707\tvalid_1's amex_metric: 0.794557\n",
      "[6500]\ttraining's binary_logloss: 0.156389\ttraining's amex_metric: 0.905451\tvalid_1's binary_logloss: 0.21547\tvalid_1's amex_metric: 0.794731\n",
      "[7000]\ttraining's binary_logloss: 0.1521\ttraining's amex_metric: 0.91232\tvalid_1's binary_logloss: 0.215258\tvalid_1's amex_metric: 0.795239\n",
      "[7500]\ttraining's binary_logloss: 0.148036\ttraining's amex_metric: 0.918902\tvalid_1's binary_logloss: 0.215105\tvalid_1's amex_metric: 0.795488\n",
      "[8000]\ttraining's binary_logloss: 0.143735\ttraining's amex_metric: 0.925813\tvalid_1's binary_logloss: 0.214963\tvalid_1's amex_metric: 0.79527\n",
      "[8500]\ttraining's binary_logloss: 0.139633\ttraining's amex_metric: 0.93204\tvalid_1's binary_logloss: 0.214938\tvalid_1's amex_metric: 0.795728\n",
      "[9000]\ttraining's binary_logloss: 0.136039\ttraining's amex_metric: 0.937583\tvalid_1's binary_logloss: 0.214908\tvalid_1's amex_metric: 0.795481\n",
      "[9500]\ttraining's binary_logloss: 0.132411\ttraining's amex_metric: 0.943331\tvalid_1's binary_logloss: 0.214818\tvalid_1's amex_metric: 0.795386\n",
      "[10000]\ttraining's binary_logloss: 0.128851\ttraining's amex_metric: 0.948314\tvalid_1's binary_logloss: 0.214818\tvalid_1's amex_metric: 0.795149\n",
      "[10500]\ttraining's binary_logloss: 0.125807\ttraining's amex_metric: 0.953057\tvalid_1's binary_logloss: 0.214799\tvalid_1's amex_metric: 0.795053\n",
      "Our fold 0 CV score is 0.7950530077529674\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1365 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.163912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224260\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1357\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.314359\ttraining's amex_metric: 0.782407\tvalid_1's binary_logloss: 0.318696\tvalid_1's amex_metric: 0.765587\n",
      "[1000]\ttraining's binary_logloss: 0.245026\ttraining's amex_metric: 0.796204\tvalid_1's binary_logloss: 0.252997\tvalid_1's amex_metric: 0.773911\n",
      "[1500]\ttraining's binary_logloss: 0.220637\ttraining's amex_metric: 0.810022\tvalid_1's binary_logloss: 0.232875\tvalid_1's amex_metric: 0.780283\n",
      "[2000]\ttraining's binary_logloss: 0.208225\ttraining's amex_metric: 0.822611\tvalid_1's binary_logloss: 0.225064\tvalid_1's amex_metric: 0.786107\n",
      "[2500]\ttraining's binary_logloss: 0.199116\ttraining's amex_metric: 0.833922\tvalid_1's binary_logloss: 0.221106\tvalid_1's amex_metric: 0.788453\n",
      "[3000]\ttraining's binary_logloss: 0.192499\ttraining's amex_metric: 0.844302\tvalid_1's binary_logloss: 0.219357\tvalid_1's amex_metric: 0.790963\n",
      "[3500]\ttraining's binary_logloss: 0.186376\ttraining's amex_metric: 0.854313\tvalid_1's binary_logloss: 0.218095\tvalid_1's amex_metric: 0.792511\n",
      "[4000]\ttraining's binary_logloss: 0.180162\ttraining's amex_metric: 0.864127\tvalid_1's binary_logloss: 0.21714\tvalid_1's amex_metric: 0.793588\n",
      "[4500]\ttraining's binary_logloss: 0.175202\ttraining's amex_metric: 0.873129\tvalid_1's binary_logloss: 0.216646\tvalid_1's amex_metric: 0.793281\n",
      "[5000]\ttraining's binary_logloss: 0.169611\ttraining's amex_metric: 0.881768\tvalid_1's binary_logloss: 0.21617\tvalid_1's amex_metric: 0.793408\n",
      "[5500]\ttraining's binary_logloss: 0.164825\ttraining's amex_metric: 0.890942\tvalid_1's binary_logloss: 0.215876\tvalid_1's amex_metric: 0.793856\n",
      "[6000]\ttraining's binary_logloss: 0.160247\ttraining's amex_metric: 0.899172\tvalid_1's binary_logloss: 0.215627\tvalid_1's amex_metric: 0.794542\n",
      "[6500]\ttraining's binary_logloss: 0.15643\ttraining's amex_metric: 0.905523\tvalid_1's binary_logloss: 0.215418\tvalid_1's amex_metric: 0.794957\n",
      "[7000]\ttraining's binary_logloss: 0.152128\ttraining's amex_metric: 0.912159\tvalid_1's binary_logloss: 0.215203\tvalid_1's amex_metric: 0.795223\n",
      "[7500]\ttraining's binary_logloss: 0.148074\ttraining's amex_metric: 0.918776\tvalid_1's binary_logloss: 0.215057\tvalid_1's amex_metric: 0.795436\n",
      "[8000]\ttraining's binary_logloss: 0.143805\ttraining's amex_metric: 0.925496\tvalid_1's binary_logloss: 0.214911\tvalid_1's amex_metric: 0.795879\n",
      "[8500]\ttraining's binary_logloss: 0.139704\ttraining's amex_metric: 0.93197\tvalid_1's binary_logloss: 0.214809\tvalid_1's amex_metric: 0.795264\n",
      "[9000]\ttraining's binary_logloss: 0.136112\ttraining's amex_metric: 0.937707\tvalid_1's binary_logloss: 0.214727\tvalid_1's amex_metric: 0.7951\n",
      "[9500]\ttraining's binary_logloss: 0.13246\ttraining's amex_metric: 0.943057\tvalid_1's binary_logloss: 0.214596\tvalid_1's amex_metric: 0.79509\n",
      "[10000]\ttraining's binary_logloss: 0.12892\ttraining's amex_metric: 0.948458\tvalid_1's binary_logloss: 0.214576\tvalid_1's amex_metric: 0.795878\n",
      "[10500]\ttraining's binary_logloss: 0.125899\ttraining's amex_metric: 0.953079\tvalid_1's binary_logloss: 0.21456\tvalid_1's amex_metric: 0.796794\n",
      "Our fold 1 CV score is 0.7967939030236229\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1365 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.103613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224219\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1357\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.314835\ttraining's amex_metric: 0.781778\tvalid_1's binary_logloss: 0.317253\tvalid_1's amex_metric: 0.768809\n",
      "[1000]\ttraining's binary_logloss: 0.245592\ttraining's amex_metric: 0.794753\tvalid_1's binary_logloss: 0.251078\tvalid_1's amex_metric: 0.77982\n",
      "[1500]\ttraining's binary_logloss: 0.221231\ttraining's amex_metric: 0.809159\tvalid_1's binary_logloss: 0.230736\tvalid_1's amex_metric: 0.785997\n",
      "[2000]\ttraining's binary_logloss: 0.208892\ttraining's amex_metric: 0.820825\tvalid_1's binary_logloss: 0.222817\tvalid_1's amex_metric: 0.79043\n",
      "[2500]\ttraining's binary_logloss: 0.199802\ttraining's amex_metric: 0.831892\tvalid_1's binary_logloss: 0.218646\tvalid_1's amex_metric: 0.794181\n",
      "[3000]\ttraining's binary_logloss: 0.193151\ttraining's amex_metric: 0.842891\tvalid_1's binary_logloss: 0.216736\tvalid_1's amex_metric: 0.79553\n",
      "[3500]\ttraining's binary_logloss: 0.187055\ttraining's amex_metric: 0.852932\tvalid_1's binary_logloss: 0.215485\tvalid_1's amex_metric: 0.795786\n",
      "[4000]\ttraining's binary_logloss: 0.18083\ttraining's amex_metric: 0.86306\tvalid_1's binary_logloss: 0.214468\tvalid_1's amex_metric: 0.797586\n",
      "[4500]\ttraining's binary_logloss: 0.1758\ttraining's amex_metric: 0.872322\tvalid_1's binary_logloss: 0.213947\tvalid_1's amex_metric: 0.798974\n",
      "[5000]\ttraining's binary_logloss: 0.170139\ttraining's amex_metric: 0.881006\tvalid_1's binary_logloss: 0.213373\tvalid_1's amex_metric: 0.799205\n",
      "[5500]\ttraining's binary_logloss: 0.165311\ttraining's amex_metric: 0.88973\tvalid_1's binary_logloss: 0.213034\tvalid_1's amex_metric: 0.799522\n",
      "[6000]\ttraining's binary_logloss: 0.160739\ttraining's amex_metric: 0.897502\tvalid_1's binary_logloss: 0.212876\tvalid_1's amex_metric: 0.800027\n",
      "[6500]\ttraining's binary_logloss: 0.156896\ttraining's amex_metric: 0.904195\tvalid_1's binary_logloss: 0.212766\tvalid_1's amex_metric: 0.800462\n",
      "[7000]\ttraining's binary_logloss: 0.152581\ttraining's amex_metric: 0.910892\tvalid_1's binary_logloss: 0.212587\tvalid_1's amex_metric: 0.80064\n",
      "[7500]\ttraining's binary_logloss: 0.148521\ttraining's amex_metric: 0.917783\tvalid_1's binary_logloss: 0.21248\tvalid_1's amex_metric: 0.800975\n",
      "[8000]\ttraining's binary_logloss: 0.144261\ttraining's amex_metric: 0.924609\tvalid_1's binary_logloss: 0.21239\tvalid_1's amex_metric: 0.800756\n",
      "[8500]\ttraining's binary_logloss: 0.140164\ttraining's amex_metric: 0.931204\tvalid_1's binary_logloss: 0.212344\tvalid_1's amex_metric: 0.800088\n",
      "[9000]\ttraining's binary_logloss: 0.136574\ttraining's amex_metric: 0.936719\tvalid_1's binary_logloss: 0.212254\tvalid_1's amex_metric: 0.800708\n",
      "[9500]\ttraining's binary_logloss: 0.132903\ttraining's amex_metric: 0.942506\tvalid_1's binary_logloss: 0.212196\tvalid_1's amex_metric: 0.800431\n",
      "[10000]\ttraining's binary_logloss: 0.129339\ttraining's amex_metric: 0.947846\tvalid_1's binary_logloss: 0.212201\tvalid_1's amex_metric: 0.800639\n",
      "[10500]\ttraining's binary_logloss: 0.126322\ttraining's amex_metric: 0.952561\tvalid_1's binary_logloss: 0.212212\tvalid_1's amex_metric: 0.800362\n",
      "Our fold 2 CV score is 0.8003621547758013\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1365 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.204359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224070\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1357\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.314679\ttraining's amex_metric: 0.781627\tvalid_1's binary_logloss: 0.317802\tvalid_1's amex_metric: 0.769719\n",
      "[1000]\ttraining's binary_logloss: 0.245317\ttraining's amex_metric: 0.795909\tvalid_1's binary_logloss: 0.251991\tvalid_1's amex_metric: 0.776469\n",
      "[1500]\ttraining's binary_logloss: 0.220831\ttraining's amex_metric: 0.809572\tvalid_1's binary_logloss: 0.231726\tvalid_1's amex_metric: 0.78329\n",
      "[2000]\ttraining's binary_logloss: 0.20842\ttraining's amex_metric: 0.822284\tvalid_1's binary_logloss: 0.224\tvalid_1's amex_metric: 0.78779\n",
      "[2500]\ttraining's binary_logloss: 0.199368\ttraining's amex_metric: 0.83329\tvalid_1's binary_logloss: 0.220158\tvalid_1's amex_metric: 0.790783\n",
      "[3000]\ttraining's binary_logloss: 0.192761\ttraining's amex_metric: 0.843679\tvalid_1's binary_logloss: 0.218482\tvalid_1's amex_metric: 0.792122\n",
      "[3500]\ttraining's binary_logloss: 0.186657\ttraining's amex_metric: 0.853118\tvalid_1's binary_logloss: 0.217228\tvalid_1's amex_metric: 0.794395\n",
      "[4000]\ttraining's binary_logloss: 0.180455\ttraining's amex_metric: 0.863009\tvalid_1's binary_logloss: 0.21632\tvalid_1's amex_metric: 0.794422\n",
      "[4500]\ttraining's binary_logloss: 0.175452\ttraining's amex_metric: 0.871942\tvalid_1's binary_logloss: 0.215817\tvalid_1's amex_metric: 0.79532\n",
      "[5000]\ttraining's binary_logloss: 0.169824\ttraining's amex_metric: 0.880822\tvalid_1's binary_logloss: 0.215222\tvalid_1's amex_metric: 0.796848\n",
      "[5500]\ttraining's binary_logloss: 0.164992\ttraining's amex_metric: 0.889934\tvalid_1's binary_logloss: 0.214952\tvalid_1's amex_metric: 0.797081\n",
      "[6000]\ttraining's binary_logloss: 0.160407\ttraining's amex_metric: 0.898008\tvalid_1's binary_logloss: 0.21466\tvalid_1's amex_metric: 0.797208\n",
      "[6500]\ttraining's binary_logloss: 0.156555\ttraining's amex_metric: 0.904574\tvalid_1's binary_logloss: 0.21452\tvalid_1's amex_metric: 0.797196\n",
      "[7000]\ttraining's binary_logloss: 0.152263\ttraining's amex_metric: 0.911763\tvalid_1's binary_logloss: 0.214382\tvalid_1's amex_metric: 0.797366\n",
      "[7500]\ttraining's binary_logloss: 0.148231\ttraining's amex_metric: 0.918238\tvalid_1's binary_logloss: 0.214241\tvalid_1's amex_metric: 0.797382\n",
      "[8000]\ttraining's binary_logloss: 0.143958\ttraining's amex_metric: 0.924903\tvalid_1's binary_logloss: 0.214219\tvalid_1's amex_metric: 0.796975\n",
      "[8500]\ttraining's binary_logloss: 0.139862\ttraining's amex_metric: 0.93178\tvalid_1's binary_logloss: 0.21414\tvalid_1's amex_metric: 0.796812\n",
      "[9000]\ttraining's binary_logloss: 0.136255\ttraining's amex_metric: 0.937796\tvalid_1's binary_logloss: 0.214135\tvalid_1's amex_metric: 0.797547\n",
      "[9500]\ttraining's binary_logloss: 0.13258\ttraining's amex_metric: 0.943494\tvalid_1's binary_logloss: 0.214126\tvalid_1's amex_metric: 0.79694\n",
      "[10000]\ttraining's binary_logloss: 0.12902\ttraining's amex_metric: 0.948732\tvalid_1's binary_logloss: 0.214121\tvalid_1's amex_metric: 0.797173\n",
      "[10500]\ttraining's binary_logloss: 0.125994\ttraining's amex_metric: 0.953058\tvalid_1's binary_logloss: 0.214159\tvalid_1's amex_metric: 0.797309\n",
      "Our fold 3 CV score is 0.7973088222360339\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1365 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.224874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224308\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1357\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.314409\ttraining's amex_metric: 0.781006\tvalid_1's binary_logloss: 0.318559\tvalid_1's amex_metric: 0.769506\n",
      "[1000]\ttraining's binary_logloss: 0.244842\ttraining's amex_metric: 0.795661\tvalid_1's binary_logloss: 0.252933\tvalid_1's amex_metric: 0.778724\n",
      "[1500]\ttraining's binary_logloss: 0.220493\ttraining's amex_metric: 0.809898\tvalid_1's binary_logloss: 0.232967\tvalid_1's amex_metric: 0.784753\n",
      "[2000]\ttraining's binary_logloss: 0.20813\ttraining's amex_metric: 0.82217\tvalid_1's binary_logloss: 0.225299\tvalid_1's amex_metric: 0.787553\n",
      "[2500]\ttraining's binary_logloss: 0.199074\ttraining's amex_metric: 0.833518\tvalid_1's binary_logloss: 0.22144\tvalid_1's amex_metric: 0.789129\n",
      "[3000]\ttraining's binary_logloss: 0.192432\ttraining's amex_metric: 0.844247\tvalid_1's binary_logloss: 0.219565\tvalid_1's amex_metric: 0.79151\n",
      "[3500]\ttraining's binary_logloss: 0.186362\ttraining's amex_metric: 0.854172\tvalid_1's binary_logloss: 0.218336\tvalid_1's amex_metric: 0.793138\n",
      "[4000]\ttraining's binary_logloss: 0.180156\ttraining's amex_metric: 0.863703\tvalid_1's binary_logloss: 0.217293\tvalid_1's amex_metric: 0.794493\n",
      "[4500]\ttraining's binary_logloss: 0.17518\ttraining's amex_metric: 0.872509\tvalid_1's binary_logloss: 0.216776\tvalid_1's amex_metric: 0.794093\n",
      "[5000]\ttraining's binary_logloss: 0.169531\ttraining's amex_metric: 0.881779\tvalid_1's binary_logloss: 0.216255\tvalid_1's amex_metric: 0.794295\n",
      "[5500]\ttraining's binary_logloss: 0.164744\ttraining's amex_metric: 0.890327\tvalid_1's binary_logloss: 0.215865\tvalid_1's amex_metric: 0.79462\n",
      "[6000]\ttraining's binary_logloss: 0.160159\ttraining's amex_metric: 0.898532\tvalid_1's binary_logloss: 0.215544\tvalid_1's amex_metric: 0.795899\n",
      "[6500]\ttraining's binary_logloss: 0.156354\ttraining's amex_metric: 0.904918\tvalid_1's binary_logloss: 0.215339\tvalid_1's amex_metric: 0.796378\n",
      "[7000]\ttraining's binary_logloss: 0.152079\ttraining's amex_metric: 0.911843\tvalid_1's binary_logloss: 0.215131\tvalid_1's amex_metric: 0.797145\n",
      "[7500]\ttraining's binary_logloss: 0.148018\ttraining's amex_metric: 0.918532\tvalid_1's binary_logloss: 0.215069\tvalid_1's amex_metric: 0.797238\n",
      "[8000]\ttraining's binary_logloss: 0.143761\ttraining's amex_metric: 0.925081\tvalid_1's binary_logloss: 0.21496\tvalid_1's amex_metric: 0.797364\n",
      "[8500]\ttraining's binary_logloss: 0.139661\ttraining's amex_metric: 0.93176\tvalid_1's binary_logloss: 0.214793\tvalid_1's amex_metric: 0.797483\n",
      "[9000]\ttraining's binary_logloss: 0.136069\ttraining's amex_metric: 0.937565\tvalid_1's binary_logloss: 0.214724\tvalid_1's amex_metric: 0.798137\n",
      "[9500]\ttraining's binary_logloss: 0.132393\ttraining's amex_metric: 0.943482\tvalid_1's binary_logloss: 0.214755\tvalid_1's amex_metric: 0.797828\n",
      "[10000]\ttraining's binary_logloss: 0.12884\ttraining's amex_metric: 0.948888\tvalid_1's binary_logloss: 0.214671\tvalid_1's amex_metric: 0.796758\n",
      "[10500]\ttraining's binary_logloss: 0.125848\ttraining's amex_metric: 0.953326\tvalid_1's binary_logloss: 0.21471\tvalid_1's amex_metric: 0.797101\n",
      "Our fold 4 CV score is 0.7971008591352851\n",
      "Our out of folds CV score is 0.7972451082119938\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train_and_evaluate(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
